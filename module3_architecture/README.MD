# ğŸ— Module 3 â€” LLM Application Architecture

This module focuses on designing a modular LLM application pipeline using clean software engineering principles.

Instead of writing everything in one file, we separate responsibilities into layers â€” similar to real-world AI systems.

---

## ğŸ¯ Objective

Build a structured LLM pipeline:

User Input â†’ Prompt Layer â†’ LLM Layer â†’ Post-Processing â†’ Output

---

## ğŸ§  Concepts Covered

- Modular AI system design
- Separation of concerns
- Prompt abstraction layer
- Model invocation layer
- Post-processing layer
- Production-style architecture

---

## ğŸ“‚ Project Structure

module3_architecture/

- input_layer.py
- prompt_layer.py
- llm_layer.py
- post_processing.py
- pipeline.py
- requirements.txt
- .env.example

---

## ğŸ— Architecture Overview

1ï¸âƒ£ Input Layer  
Handles user interaction.

2ï¸âƒ£ Prompt Layer  
Builds structured prompts.

3ï¸âƒ£ LLM Layer  
Sends request to Groq via LiteLLM.

4ï¸âƒ£ Post Processing  
Cleans and formats output.

5ï¸âƒ£ Pipeline  
Orchestrates the entire workflow.

---

## âš™ Installation

### 1. Create Virtual Environment

Windows:
python -m venv venv
venv\Scripts\activate
---

### 2. Install Dependencies


pip install -r requirements.txt


---

### 3. Add Environment Variables

Create a `.env` file:


GROQ_API_KEY=your_key_here
MODEL_NAME=groq/llama-3.1-8b-instant


Do NOT upload `.env` to GitHub.

---

## â–¶ Run the Project


python pipeline.py


---

## ğŸ“ Learning Outcome

After completing this module, students will:

- Understand AI workflow design
- Build modular LLM systems
- Separate prompt logic from model logic
- Design scalable GenAI applications

---

## ğŸš€ Why This Matters

Real-world AI systems are NOT single scripts.

They are modular pipelines with:
- Input validation
- Prompt engineering
- Model abstraction
- Output processing

This module builds that engineering mindset.